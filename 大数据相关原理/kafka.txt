
生产者
	负载均衡:
		生产者可以不经过任何路由直接发送数据到broker的leader partition。
		为了做到这一点所有的broker都能提供元数据给生产者，告诉它哪些服务器还活着，leader partition在哪儿。
		生产者可以控制数据发送到哪个partition。它可以随机发送，也可以实现一种随机负载均衡发送，也可以通过实现分区函数来完成
		（默认为hash分区函数）。发送数据时可以指定key，根据key进行hash来发送到对应的分区。

	异步发送:
		批量发送能大幅度提升效率。kafka生产者可以将数据收集在内存。一次请求把这一批直接发过来。这个批量设置可以设置为
		收集了多少条message或者收集了多久的时间。

消费者
	push VS pull:
	我们首先考虑的问题是，consumers应该从broker中pull数据，还是broker应该将数据push给consumers。
	在这方面，Kafka采用了一种更传统的设计，大多数消息传递系统都是这种设计，其中数据由producer推送到broker，
	由consumers从broker中pull
	在push的系统中。因为broker控制数据传输的数据,所以很难对接不同的consumers。目标通常是让消费者能够以最大可能的速度消费;
	不幸的是，在push系统中，这意味着当消费率低于生产率时(本质上是拒绝服务攻击)，consumer往往会不知所措
	基于pull的系统具有更好的特性，消费者可以落后于它，并在可能的时候赶上它。
	可以通过某种后退协议来缓解这种情况，通过这种协议,表明consumer已经不堪重负，
	但是要让传输速率充分利用(但绝不是过度利用)consumer要比看起来复杂得多。
	kafka采用了更传统的pull模型。
	
	基于pull的系统的另一个优点是，它可以积极地批量处理发送给consumer的数据。基于push的系统必须选择立即发送一个请求，或者积累更多的数据，然后稍后发送，
	而不知道下游消费者是否能够立即处理它。如果调优为低延迟，这将导致每次只发送一条消息，最终只缓冲传输，这是浪费。基于pull的设计解决了这一问题，
	因为使用者总是在日志中当前位置之后(或达到某些可配置的最大大小)提取所有可用的消息。因此可以在不引入不必要延迟的情况下获得最佳批处理。

	简单的基于pull的系统的不足之处在于，如果broker没有数据，那么使用者可能最终会在一个紧密的循环中轮询，从而有效地忙于等待数据的到来。
	为了避免这种情况，我们在pull请求中使用了一些参数，这些参数允许使用者请求阻塞在一个“长轮询”中，
	等待数据到达(也可以选择等待给定的字节数，以确保传输大小足够大)。
	
	消费位置:
	令人惊讶的是，跟踪已消费的内容是消息传递系统的关键性能点之一。
	
	大多数消息传递系统将哪些消息被消费的元数据保存在在broker上。也就是说，当消息传递给consumer时，broker要么立即在本地记录该事件，要么等待消费者的确认。
	这是一个相当直观的选择，而且对于单个机器服务器来说，还不清楚这种状态还可能出现在何处。由于许多消息传递系统中用于存储的数据结构伸缩性很差，
	所以这也是一个实用的选择——因为boker知道使用了什么，所以它可以立即删除它，从而保持数据的大小较小。

	可能不太明显的是，让broker和consumer就所消费的内容达成一致并不是一个简单的问题。如果broker在每次通过网络分发消息时都将其立即记录为已消费的消息，
	那么如果consumer未能处理该消息(例如因为它崩溃或请求超时或其他原因)，则该消息将丢失。为了解决这个问题，许多消息传递系统添加了一个确认功能，
	这意味着消息只在发送时被标记为已发送而不是已使用;broker等待来自consumer的特定确认，以便在使用消息时记录消息。这种策略解决了丢失消息的问题，但也产生了新的问题。
	首先，如果consumer处理消息，但在发送确认之前失败，则该消息将被使用两次。第二个问题与性能有关，现在broker必须对每个消息保持多个状态
	(首先锁定它，这样就不会第二次发出消息，然后将其标记为永久使用，以便删除消息)。
	必须处理棘手的问题，比如如何处理发送但从未被回应的消息。
		
	Kafka以不同的方式处理这个问题。我们的主题被划分为一组完全有序的分区，每个分区在任何给定时间都由每个订阅用户组中的一个用户使用。
	这意味着消费者在每个分区中的位置只是一个整数，即下一个要消费的消息的偏移量。这使得消耗的状态非常小，每个分区只有一个数字。
	这种状态可以定期检查。这使得等价的消息确认非常便宜。

	这个决定还有一个好处。使用者可以有意地倒回旧的偏移量并重新使用数据。这违反了队列的公共契约，但实际上是许多消费者的一个基本特性
	例如，如果使用者代码有一个错误，并且在使用了一些消息之后发现了它，那么一旦错误被修复，使用者就可以重新使用这些消息。
	
	离线数据加载
	可伸缩持久性允许只定期使用批处理数据加载(批处理数据加载会定期将数据批量加载到离线系统，如Hadoop或关系数据仓库)的使用者。

	在Hadoop的情况下，我们通过将负载拆分到各个map任务(每个节点/主题/分区组合对应一个映射任务)来并行化数据负载，从而允许在加载过程中完全并行。
	Hadoop提供任务管理，失败的任务可以重新启动，而不会有重复数据的危险——它们只是从原始位置重新启动。

消息分发语义
	既然我们已经了解了生产者和消费者如何工作，那么让我们讨论一下Kafka在生产者和消费者之间提供的语义保证。
	显然，可以提供多种可能的消息传递保证:
		最多一次（At most once）-------------消息可能丢失，但永远不会重新传递。
		至少有一次（At least once）----------消息永远不会丢失，但是可以重新传递。
		Exactly once-------------------------这就是人们真正想要的，每条信息只传递一次。
	值得注意的是，这分为两个问题:发布消息的持久性保证和消费消息的保证
	许多系统声称提供了“精确的一次”交付语义，但是阅读细则是很重要的，这些说法大多是误导
	（也就是说，它们不会涉及到消费者或生产者可能失败的情况、存在多个消费者进程的情况、或写入磁盘的数据可能丢失的情况）
	
	
	kafka的语义学是直截了当的。当发布消息时，我们有消息被“committed”到日志的概念。一旦发布的消息被committed，只要写入的该消息partition的broker保持“alive”
	，该消息就不会丢失。下一节将更详细地描述committed message的定义、alive partition以及我们试图处理的故障类型。
	现在，让我们假设一个完美的、无损的broker，并尝试理解对生产者和消费者的担保。
	如果生产者试图发布一条消息发生了网络错误，它不能确定这个错误是在committed message之前还是之后发生的。这类似于使用自动生成的键插入数据库表的语义。
	
	
	在0.11.0.0之前，如果生产者未能接收到指示消息已提交的响应，那么它别无选择，只能重新发送消息。这至少提供了一次传递语义（ at-least-once ），因为如果原始请求确实成功了，
	则在重新发送期间可以再次将消息写入日志。从0.11.0.0开始，Kafka生成器还支持幂等传递选项，该选项保证重新发送不会导致日志中出现重复内容。
	为此，broker为每个producer分配一个ID，并使用生产者连同每个消息一起发送的序列号来删除重复消息。
	同样从0.11.0.0开始，生产者支持使用类似事务的语义将消息发送到多个主题分区的能力:即要么所有消息都被成功写入，要么没有一条消息被成功写入。
	这种方法的主要用例是在Kafka主题之间进行exactly-once(如下所述)。
	
	并不是所有的场景都需要这样强大的保证。对于延迟敏感的应用，我们允许生产者指定它希望的持久性级别。
	如果生产者指定它想要等待提交的消息，这可能需要10毫秒的时间。然而，生产者也可以指定它想要完全异步地执行发送，
	或者它只想等到领导者(但不一定是追随者)得到消息。ack
	
	现在让我们从消费者的角度来描述语义。所有副本具有完全相同的日志，具有相同的偏移量。使用者控制其在此日志中的位置。
	如果使用者没有崩溃，它可以将这个位置存储在内存中，但是如果使用者失败，
	并且我们希望这个主题分区被另一个进程接管，那么新进程将需要选择一个合适的位置，从那里开始处理。
	假设使用者读取一些消息——它有几个选项来处理消息并更新其位置。
		1.它可以读取消息，然后保存其在日志中的位置，最后处理消息。在这种情况下，使用者进程有可能在保存其位置之后但在保存其消息处理的输出之前崩溃。
		在这种情况下，接管处理的进程将从保存的位置开始，即使该位置之前的一些消息没有被处理。
		这对应于“At most once”语义，因为在消费者失败消息可能不被处理的情况下。
		2.它可以读取消息，处理消息，最后保存其位置。在这种情况下，使用者进程有可能在处理消息之后但在保存其位置之前崩溃。
		在这种情况下，当新进程接收到的头几条消息时，它已经处理好了。这对应于消费者失败情况下的“At least once”语义。
		在许多情况下，消息都有一个主键，因此更新是幂等的(两次接收相同的消息只会用它自身的另一个副本覆盖一条记录)。
	那么什么是exactly once语义呢（也就是你真正想要的东西）。当从Kafka主题消费并生成到另一个主题时(就像在Kafka Streams应用程序中一样)，
	我们可以利用上面提到的0.11.0.0中的新的事务生成器功能。使用者的位置存储为主题中的消息。如果事务被中止，使用者的位置将恢复到原来的值，
	并且关于输出主题的生成数据对其他使用者不可见，这取决于他们的“隔离级别”。
	在默认的“read_uncommitted”隔离级别中，所有消息对使用者都是可见的，即使它们是中止事务的一部分，
	但在“read_committed”中，消费者将只消费已经提交事务的消息(以及不属于事务的任何消息)。
	
	当写入外部系统时，限制在于需要将使用者的位置与实际存储为输出的内容进行协调。实现这一点的经典方法是在使用者位置的存储和使用者输出的存储之间引入两阶段提交。
	但是，这可以通过让消费者将其偏移量存储在与输出相同的位置来更简单地处理。这更好，因为消费者可能希望写入的许多输出系统不支持两阶段提交。
	考虑一个Kafka连接连接器，它在HDFS中填充数据和它读取的数据的偏移量，这样就可以保证数据和偏移量都是更新的，或者两者都不是。
	对于许多其他数据系统，我们遵循类似的模式，这些系统需要更强的语义，并且消息没有允许重复数据删除的主键。
	因此，Kafka有效地支持Kafka流中的精确一次交付，并且当在Kafka主题之间传输和处理数据时，事务生产者/消费者通常可以用来提供精确一次交付。
	为其他目标系统提供一次交付通常需要与此类系统协作，但是Kafka提供了实现此功能的补偿(请参阅Kafka Connect)。
	否则，Kafka默认情况下保证至少一次交付，并允许用户实现最多一次交付，方法是在处理一批消息之前禁用对生产者的重试并在消费者中提交偏移量。
	
4.7 副本
	Kafka跨多个可配置的服务器复制每个主题分区的日志（您可以根据每个主题设置此复制因子）。这允许在集群中的服务器发生故障时自动将故障转移到这些副本，以便在出现故障时消息仍然可用。
	
	其他消息传递系统提供了一些与复制相关的功能，但是，在我们(完全有偏见的)看来，这似乎是一个附加的功能，并没有大量使用，
	而且有很大的缺点:副本是不活动的，吞吐量受到严重影响，它需要手工配置等。Kafka默认情况下用于复制——事实上，我们将未复制的主题实现为复制主题，其中复制因子为1。
	
	复制的单元是主题分区。在非故障条件下，Kafka中的每个分区都有一个leader和零个或多个follower。包括leader在内的副本总数构成复制因子。所有的读和写都归分区的leader。
	通常，分区比broker多得多，并且在broker之间均匀地分布着leader。追随者的日志与领导者的日志相同——都具有相同的偏移量和相同的消息顺序（当然，在任何给定的时间，leader的日志末尾都可能有一些尚未复制的消息）
	跟随者使用来自领导者的消息，就像普通的Kafka使用者一样，并将它们应用到自己的日志中。让跟随者从领导者那里拉出有一个很好的特性，允许跟随者自然地将它们应用到日志中的日志条目批处理在一起。
	
	Followers 使用来自 leader 的消息，就像普通的Kafka consumer一样，并将它们添加到自己的日志中。让Followers从leader那里拉出有一个很好的特性，
	允许跟随者自然地将它们应用到日志中的日志条目批处理在一起。与大多数分布式系统一样，自动处理故障需要精确定义节点“active”的含义。
	对于Kafka节点，active有两个条件
		1.节点必须能够维护与ZooKeeper的会话(通过ZooKeeper的心跳机制)
		2.如果它是一个追随者，它必须复制发生在领导者身上的写操作，而且不能“太落后”
	我们将满足这两个条件的节点称为“in sync”节点，以避免“active”或“failed”的模糊性。领导者跟踪一组“in sync”节点。如果一个follower死了，卡住了，或者落在了后面，
	领导者将把它从同步副本列表中删除，确定被卡住和滞后的副本由replica.lag.time.max.ms配置。
	
	在分布式系统术语中，我们只尝试处理“故障/恢复”故障模型，其中节点突然停止工作，然后恢复(可能不知道它们已经死亡)。
	Kafka不处理所谓的“拜占庭式”故障，在这种故障中，节点产生任意或恶意的响应(可能是由于bug或恶意操作)。
	
	现在，我们可以更精确地定义，当该分区的所有同步副本都将消息应用到它们的日志中时，就认为该消息已提交。只有提交的消息才会发送给消费者。这意味着，如果领导者失败，消费者不必担心可能会看到丢失的消息。
	另一方面，生产者可以选择等待或不提交消息，这取决于他们对延迟和持久性之间权衡的偏好。此首选项由生成器使用的acks设置控制。
	注意，主题有一个同步副本“最小数量”的设置，当生产者请求确认消息已被写入完整的同步副本时，将检查该设置。
	
	如果生产者请求不那么严格的确认，则可以提交和使用消息，即使同步副本的数量低于最小值(例如，它可以和leader一样低)。
	
	Kafka提供的保证是，提交的消息不会丢失，只要有至少一个同步副本一直存在。在短的故障转移期间之后，Kafka在节点出现故障时仍然可用，但在网络分区出现时可能不可用。
	
复制日志:Quorums、ISRs和状态机(哦，天哪!)
	Kafka分区的核心是一个副本。副本是分布式数据系统中最基本的原语之一，有许多实现方法。副本可以被其他系统用作以状态机方式实现其他分布式系统的基本元素。
	
	副本对一系列值的顺序达成一致意见的过程建模(通常为副本条目编号0、1、2、…)。有很多方法可以实现这一点，
	但是最简单和最快的方法是由一个选择提供给它的值的顺序的领导者来实现。
	只要领导者还活着，所有的followers只需要顺着leader值的顺序来复制值。
	
	当然，如果leaders没有挂掉，我们就不需要followers了!当leaders 挂掉时，我们需要从followers中选择一个新的leaders。
	但是followers本身可能会落后或崩溃，所以我们必须确保选择一个最新的followers。日志复制算法必须提供的基本保证是，如果我们告诉客户端提交了一条消息，而leader失败了，
	那么我们选择的新leader也必须拥有该消息。这就产生了一个权衡:如果领导者等待更多的追随者确认消息，然后才宣布其已提交，那么就会有更多潜在的可选领导者。
	
	如果您选择了所需的确认数量和必须进行比较的日志数量，才能选出一个确保有重叠的leader，那么这就称为Quorum（法定人数 仲裁 会议法定人数）。
	
	这种权衡的一种常见方法是对提交决策和领导人选举都使用多数票。这并不是Kafka所做的，但是让我们来探索一下它，以理解其中的权衡。
	假设有2f+1个副本。如果f+1副本必须在leader声明commit之前接收到一条消息，
	并且如果我们通过从至少f+1副本中选择日志最完整的follower来选举一个新的leader，
	然后，在失败不超过f的情况下，保证leader拥有所有提交的消息。这是因为在任何f+1副本中，必须至少有一个副本包含所有提交的消息。
	该副本的日志将是最完整的，因此将被选择为新的leader。
	每种算法都必须处理许多剩余的细节(比如精确定义了什么使日志更完整，确保在leader故障期间日志的一致性，或者更改副本集中的服务器集)，
	但是现在我们将忽略这些细节。
	
	这种多数表决方法有一个非常好的特性:延迟只依赖于速度最快的服务器。也就是说，如果复制因子为3，则延迟由速度更快的追随者决定，而不是由速度较慢的追随者决定。
	
	这个家族中有很多算法，包括ZooKeeper的Zab、Raft和viewstamp复制。我们所知道的与Kafka的实际实现最相似的学术出版物是来自微软的PacificA。
	
	多数人投票的缺点是，很多次失败并不会让你失去一个可以选举的领导人。要容忍一个故障需要三份数据副本，而容忍两个故障则需要五份数据副本。
	根据我们的经验，仅有足够的冗余来容忍单个故障对于实际的系统来说是不够的，但是每次写5次，磁盘空间需求是5倍，吞吐量是1/5，对于大容量数据问题来说是不太实际的。
	这可能就是为什么quorum算法在共享集群配置(如ZooKeeper)中更常见，而在主数据存储中则不太常见的原因。例如，在HDFS中，namenode的高可用性特性是基于基于多数选票的日志构建的，
	但是这种更昂贵的方法并不用于数据本身。
	
	Kafka在选择它的quorum set时采用了一种稍微不同的方法。Kafka动态地维护一组同步副本(ISR)，这些副本将被提交给领导者，而不是多数人投票。
	只有这一组成员才有资格当选为领导人。在所有同步的副本都收到写操作之前，对Kafka分区的写操作不会被认为是提交的。
	当ISR集发生变化时，它将被持久化到ZooKeeper。因此，ISR中的任何复制品都有资格被选为领导人。
	对于Kafka的使用模型来说，这是一个重要的因素，因为有很多分区，确保领导层的平衡很重要。
	使用这个ISR模型和f+1副本，Kafka主题可以容忍f失败而不会丢失提交的消息。
	
	对于我们希望处理的大多数用例，我们认为这种权衡是合理的。在实践中，为了容忍f失败，大多数投票和ISR方法都将等待相同数量的副本在提交消息之前得到确认
	(例如，要在一次失败中存活，多数仲裁需要三个副本和一个确认，而ISR方法需要两个副本和一个确认)。不使用最慢的服务器提交的能力是多数投票方法的优势。
	但是，我们认为允许客户端选择是否阻塞消息提交可以改善这种情况，而且由于所需的复制因子较低，因此额外的吞吐量和磁盘空间是值得的。
	
	另一个重要的设计区别是Kafka不要求崩溃的节点在所有数据完整的情况下恢复。在这个空间中，复制算法依赖于“稳定存储”的存在是很常见的，
	在任何故障恢复场景中，如果没有潜在的一致性冲突，“稳定存储”是不会丢失的。这个假设有两个主要问题。
	首先，磁盘错误是我们在持久数据系统的实际操作中观察到的最常见的问题，它们通常不会保持数据的完整性。
	其次，即使这不是一个问题，我们也不希望为了保证一致性而要求每次写入都使用fsync，因为这会将性能降低2到3个数量级。
	我们允许副本重新加入ISR的协议确保在重新加入之前，它必须完全重新同步，即使它在崩溃中丢失了未刷新的数据。
	
不洁领袖选举:如果他们都死了怎么办?
	注意，Kafka对数据丢失的保证是基于至少一个保持同步的副本。如果复制分区的所有节点都死亡，则此保证不再有效。
	然而，一个实际的系统需要在所有副本都死后做一些合理的事情。如果你很不幸发生了这样的事情，考虑一下将会发生什么是很重要的。有两种行为可以实现:
		1.等待ISR中的一个副本复活并选择这个副本作为leader(希望它仍然拥有所有的数据)。
		2.选择第一个副本(不一定是在ISR中)，作为leader返回到生命中。
	这是可用性和一致性之间的简单权衡。如果我们在ISR中等待副本，那么只要这些副本关闭，我们就仍然不可用。
	如果这样的副本被销毁或者它们的数据丢失了，那么我们将永远处于崩溃状态。
	另一方面，如果一个不同步的副本恢复了生命，并且我们允许它成为领导者，那么它的日志将成为真相的来源，即使它不能保证拥有所有提交的消息。
	默认情况下，从0.11.0.0版本开始，Kafka选择第一种策略，并倾向于等待一致的副本。可以使用configuration属性unclean.leader.election更改此行为。
	支持正常运行时间优于一致性的用例。
	
	这种困境并非卡夫卡独有。它存在于任何基于quorum的方案中。例如，在多数投票方案中，如果大多数服务器出现永久性故障，
	那么您必须选择丢失100%的数据，或者将现有服务器上的剩余数据作为新的真相来源，从而破坏一致性。
	
	
可用性和耐久性保证	
	在编写Kafka时，生产者可以选择等待消息被0、1或所有(-1)副本确认。注意，“所有副本的确认”并不保证所有已分配的副本都已收到消息。
	默认情况下，当ack =all时，当所有当前同步的副本都收到消息时，就会发生确认。
	例如，如果一个主题只配置了两个副本，并且其中一个副本失败(即，则写入指定acks=all将成功。但是，如果剩余的副本也失败，这些写操作可能会丢失。
	尽管这确保了分区的最大可用性，但是对于一些喜欢持久性而不是可用性的用户来说，这种行为可能是不受欢迎的。
	因此，我们提供了两个主题级别的配置，可以用来选择消息的持久性，而不是可用性:
		1.禁用不洁领袖选举——如果所有副本都不可用，那么分区将一直不可用，直到最近的领袖再次可用。
		这实际上更倾向于不可用性，而不是消息丢失的风险。请参阅前一节关于不洁领袖选举的说明。
		2.指定ISR的最小大小——只有当ISR的大小超过某个最小值时，分区才会接受写操作，以防止丢失仅写入一个副本的消息，该副本随后变得不可用。
		只有当生产者使用acks=all并保证消息将被至少这么多同步副本确认时，此设置才会生效。
		该设置提供了一致性和可用性之间的权衡。为最小ISR大小设置较高的值可以保证更好的一致性，因为可以保证将消息写入更多的副本wh
	
复制管理
	上面关于复制日志的讨论实际上只涉及一个日志，即一个主题分区。但是Kafka集群将管理成百上千个分区。我们尝试以循环方式平衡集群中的分区，
	以避免在少数节点上为高容量主题聚集所有分区。同样，我们尝试平衡领导权，使每个节点都是其分区的按比例分配的领导者。
	
	优化领导人选举过程也很重要，因为这是不可用的关键窗口。一个简单的leader election实现最终会对一个节点失败时承载的所有分区的每个分区运行一个选举。
	相反，我们选择其中一个代理作为“控制器”。该控制器检测代理级别上的故障，并负责更改失败代理中所有受影响分区的leader。
	其结果是，我们能够批量处理许多所需的领导层变更通知，这使得对于大量分区来说，选举过程要便宜得多，速度也快得多。如果控制器失败，幸存的代理之一将成为新控制器。
	
日志合并
	日志合并确保Kafka将始终为单个主题分区的数据日志中的每个消息键保留至少最后一个已知值。
	它处理用例和场景，比如在应用程序崩溃或系统故障后恢复状态，或者在运行维护期间应用程序重启后重新加载缓存。让我们更详细地研究这些用例，然后描述压缩是如何工作的。
	
	到目前为止，我们只描述了一种更简单的数据保留方法，即在一段固定时间之后或当日志达到某个预定大小时丢弃旧的日志数据。
	这对于时间事件数据非常有效，比如记录每条记录单独存在的日志记录。然而，数据流的一个重要类是对键控可变数据(例如，对数据库表的更改)的更改日志。
	让我们讨论这样一个流的具体例子。假设我们有一个包含用户电子邮件地址的主题;
	每次用户更新其电子邮件地址时，我们都会使用用户id作为主键向该主题发送一条消息。现在，假设我们在一段时间内为id为123的用户发送以下消息，
	每个消息对应于电子邮件地址的更改(省略其他id的消息):
	123 => bill@microsoft.com
        .
        .
        .
	123 => bill@gatesfoundation.org
        .
        .
        .
	123 => bill@gmail.com
	日志压缩为我们提供了一个更细粒度的保留机制，这样我们就可以保证至少保留每个主键的最新更新(例如bill@gmail.com)。
	通过这样做，我们保证日志包含每个键的最终值的完整快照，而不只是最近更改的键。这意味着下游消费者可以从这个主题恢复他们自己的状态，而不需要我们保留所有更改的完整日志。
	让我们从几个有用的用例开始，然后看看如何使用它。
		1.数据库更改订阅。在多个数据系统中通常需要有一个数据集，其中一个系统通常是某种类型的数据库(RDBMS或一种新颖的键值存储)。
		例如，您可能有一个数据库、一个缓存、一个搜索集群和一个Hadoop集群。对数据库的每次更改都需要反映在缓存、搜索集群以及Hadoop中。
		在只处理实时更新的情况下，您只需要最近的日志。但是，如果希望能够重新加载缓存或恢复失败的搜索节点，则可能需要完整的数据集。
		2.事件采购。这是一种应用程序设计风格，它将查询处理与应用程序设计放在一起，并使用更改日志作为应用程序的主要存储。
		3.高可用性的日志记录。执行本地计算的进程可以通过记录对其本地状态所做的更改来实现容错，这样其他进程就可以重新加载这些更改，并在发生故障时继续执行。
		一个具体的例子是在流查询系统中处理计数、聚合和其他类似“group by”的处理。Samza是一个实时流处理框架，它正是为此目的而使用这个特性。
	在这些情况下，我们主要需要处理更改的实时提要，但是有时候，当机器崩溃或需要重新加载或重新处理数据时，我们需要完全加载。日志压缩允许从相同的支持主题提供这两个用例。
	日志的这种使用方式在这篇博客文章中有更详细的描述。
	
	总的思路很简单。如果我们有无限的日志保留，并且在上面的例子中记录了每次更改，那么我们就可以在每次系统开始时捕获它的状态。使用这个完整的日志，
	我们可以通过重放日志中的前N条记录来恢复到任何时间点。这种假设的完整日志对于多次更新单个记录的系统来说不太实用，因为即使对于稳定的数据集，日志也会无限制地增长。
	丢弃旧更新的简单日志保留机制将限制空间，但日志不再是恢复当前状态的方法—从日志开始恢复不再重新创建当前状态，因为旧更新可能根本不会被捕获。
	
	日志压缩是一种机制，用于为每条记录提供更细粒度的保留，而不是基于时间的粗粒度保留。其思想是有选择地删除具有相同主键的最新更新的记录。这样，日志就保证每个键至少具有最后的状态。
	
	可以为每个主题设置此保留策略，因此单个集群可以拥有一些主题，其中保留是通过大小或时间强制的，而其他主题是通过压缩强制的
	
	这个功能的灵感来自LinkedIn最古老和最成功的基础设施之一——一个名为Databus的数据库变更日志缓存服务。与大多数日志结构的存储系统不同，Kafka是为订阅而构建的，并为快速的线性读写组织数据。
	与Databus不同，Kafka充当真实源存储，因此即使在上游数据源无法重新播放的情况下，Kafka也是有用的。
	
日志压缩基础
	
	
	





4.9配额
	Kafka集群能够对请求强制执行配额，以控制客户端使用的代理资源。Kafka代理可以为共享配额的每一组客户执行两种类型的客户配额:
	1.网络带宽配额定义字节率阈值(自0.9起)
	2.请求速率配额将CPU利用率阈值定义为网络和I/O线程的百分比(从0.11开始)
	
为什么需要配额?
	生产者和消费者有可能生产/消费非常高的数据量或以非常高的速率生成请求，从而垄断代理资源，导致网络饱和，并通常拒绝服务其他客户端和代理本身。
	拥有配额可以防止这些问题的发生，在大型多租户集群中，这一点尤为重要。在大型集群中，一小组表现不佳的客户机可以降低表现良好的客户机的用户体验。
	实际上，当将Kafka作为服务运行时，甚至可以根据约定的契约强制执行API限制。
	
客户群体
	Kafka客户机的标识是用户主体，它表示安全集群中经过身份验证的用户。在支持未经身份验证的客户机的集群中，user principal是一组未经身份验证的用户，
	由代理使用可配置的PrincipalBuilder选择。客户机id是由客户机应用程序选择有意义的名称的客户机逻辑分组。元组(用户、客户机id)定义了一组安全的逻辑客户机，它们共享用户主体和客户机id。
	
	配额可以应用于(用户、客户端id)、用户或客户端id组。对于给定的连接，应用与该连接匹配的最特定配额。
	配额组的所有连接共享为该组配置的配额。例如，如果(user="test-user"， client-id="test-client")有一个10MB/秒的生成配额，
	那么这个配额将在用户"test-user"的所有生成器实例中与客户机id "test-client"共享。
	
配额配置	
	可以为(用户、客户端id)、用户和客户端id组定义配额配置。可以在需要更高(甚至更低)配额的任何配额级别上覆盖缺省配额。该机制类似于每个主题的日志配置覆盖。
	User和(User, client-id)配额覆盖在/config/users下写入ZooKeeper，而客户机id配额覆盖在/config/clients下写入。这些覆盖被所有代理读取，并立即生效。
	这允许我们更改配额，而不必重新启动整个集群。详情请看这里。每个组的默认配额也可以使用相同的机制动态更新。
	
	配额配置的优先顺序为:
		/config/users/<user>/clients/<client-id>
		/config/users/<user>/clients/<default>
		/config/users/<user>
		/config/users/<default>/clients/<client-id>
		/config/users/<default>/clients/<default>
		/config/users/<default>
		/config/clients/<client-id>
		/config/clients/<default>
		
	还可以使用代理属性(quota.producer.default、quota.consumer.default)为客户机id组设置默认的网络带宽配额。这些属性正在被弃用，将在稍后的版本中删除。
	客户端id的默认配额可以在Zookeeper中设置，类似于其他配额覆盖和默认设置。
	
	网络带宽配额
	
	网络带宽配额定义为共享配额的每组客户机的字节速率阈值。默认情况下，每个惟一的客户端组接收一个固定的配额，单位是字节/秒，由集群配置。
	这个配额是在每个代理的基础上定义的。在对客户机进行节流之前，每组客户机可以为每个代理发布/获取最大X字节/秒。
	
	请求速率配额
	
	请求速率配额定义为客户机可以在配额窗口内对每个代理的请求处理程序I/O线程和网络线程使用的时间百分比。
	配额n%表示一个线程的n%，因此配额超出了((number .io)的总容量。线程+ .net .threads) * 100%)%。
	在配额窗口中，每组客户机在被节流之前，可以跨所有I/O和网络线程使用最高为n%的总百分比。
	由于分配给I/O和网络线程的线程数量通常基于代理主机上可用的内核数量，因此请求速率配额表示共享配额的每组客户机可能使用的CPU总数百分比。
	
Enforcement
	
	
	
	
	
	
实现

网络层
	网络层是一个相当简单的NIO服务器，不会详细描述。
	sendfile实现是通过给MessageSet接口一个writeTo方法来实现的。这允许文件支持的消息集使用更有效的transferTo实现，而不是进程内缓冲写。
	
	网络层是一个相当简单的NIO服务器，不会详细描述。
	sendfile实现是通过给MessageSet接口一个writeTo方法来实现的。这允许文件支持的消息集使用更有效的transferTo实现，而不是进程内缓冲写。

消息
	消息由可变长度头、可变长度不透明键字节数组和可变长度不透明值字节数组组成。下一节将描述标题的格式。
	保持键和值不透明是正确的决定:目前在序列化库方面已经取得了很大的进展，任何特定的选择都不太可能适用于所有用途。
	不用说，使用Kafka的特定应用程序很可能要求使用特定的序列化类型。
	RecordBatch接口只是消息上的一个迭代器，使用专门的方法对NIO通道进行批量读写

消息格式
	消息(aka记录)总是成批写入。消息批处理的技术术语是记录批处理，记录批处理包含一个或多个记录。
	在简并的情况下，我们可以有一个包含单个记录的记录批。记录批和记录有它们自己的头。每种格式如下所述。
	
Record Batch
	











安全


	
	
	
	
	
	
	
	
	
	
	
	
	


